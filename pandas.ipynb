{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6075433-678c-4948-be4d-383afced9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29498c-bfc5-4d3f-a5c2-cc5994a3c480",
   "metadata": {},
   "source": [
    "# understanding DataFrames and Series, the two primary data structures in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1622d700-ef59-46f5-a85a-54835476829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "dtype: int64\n",
      "      Name  Age\n",
      "0    Navya   28\n",
      "1  Sanjana   38\n",
      "2   Harika   60\n"
     ]
    }
   ],
   "source": [
    "series=pd.Series([1, 2, 3, 4, 5])\n",
    "print(series)\n",
    "# DataFrame\n",
    "data=[['Navya',28],['Sanjana',38],['Harika',60]]\n",
    "df=pd.DataFrame(data, columns=['Name','Age'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa5bbe-a78c-4a55-b22b-b57cd7804bd6",
   "metadata": {},
   "source": [
    "# reading from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6cf15cb-7f18-4c95-bf3d-e7618da16b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Employee_details_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a835f9-afd7-46db-bf6e-a3f4d9bd8089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Education  JoiningYear       City  PaymentTier  Age  Gender EverBenched  \\\n",
      "0     Bachelors         2017  Bangalore            3   34    Male          No   \n",
      "1     Bachelors         2013       Pune            1   28  Female          No   \n",
      "2     Bachelors         2014  New Delhi            3   38  Female          No   \n",
      "3       Masters         2016  Bangalore            3   27    Male          No   \n",
      "4       Masters         2017       Pune            3   24    Male         Yes   \n",
      "...         ...          ...        ...          ...  ...     ...         ...   \n",
      "4648  Bachelors         2013  Bangalore            3   26  Female          No   \n",
      "4649    Masters         2013       Pune            2   37    Male          No   \n",
      "4650    Masters         2018  New Delhi            3   27    Male          No   \n",
      "4651  Bachelors         2012  Bangalore            3   30    Male         Yes   \n",
      "4652  Bachelors         2015  Bangalore            3   33    Male         Yes   \n",
      "\n",
      "      ExperienceInCurrentDomain  LeaveOrNot  \n",
      "0                             0           0  \n",
      "1                             3           1  \n",
      "2                             2           0  \n",
      "3                             5           1  \n",
      "4                             2           1  \n",
      "...                         ...         ...  \n",
      "4648                          4           0  \n",
      "4649                          2           1  \n",
      "4650                          5           1  \n",
      "4651                          2           0  \n",
      "4652                          4           0  \n",
      "\n",
      "[4653 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d5505-1e91-42ef-b78a-d66bb9d96d27",
   "metadata": {},
   "source": [
    "# operations such as selecting data, filtering rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "326a5bcb-10a3-4843-84aa-7f373173bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      rohit\n",
      "1    karthik\n",
      "2       uday\n",
      "Name: Name, dtype: object\n",
      "      Name  Age\n",
      "0    rohit   24\n",
      "1  karthik   27\n",
      "2     uday   22\n",
      "Name    rohit\n",
      "Age        24\n",
      "Name: 0, dtype: object\n",
      "Name    uday\n",
      "Age       22\n",
      "Name: 2, dtype: object\n",
      "      Name  Age\n",
      "1  karthik   27\n"
     ]
    }
   ],
   "source": [
    "data={'Name': ['rohit', 'karthik', 'uday'], 'Age': [24, 27, 22]}\n",
    "df=pd.DataFrame(data)\n",
    "# Selecting a column\n",
    "print(df['Name'])\n",
    "# Selecting multiple columns\n",
    "print(df[['Name', 'Age']])\n",
    "# Selecting rows by index\n",
    "print(df.iloc[0])\n",
    "print(df.iloc[-1])\n",
    "# Filtering rows\n",
    "filter=df[df['Age'] > 25]\n",
    "print(filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012232c1-92e9-4207-9181-66af1977d4d1",
   "metadata": {},
   "source": [
    "#  Data Handling with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "155e8753-3db0-420e-89bb-9e8836a55717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before handling missing values:\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "0            892         0       3   \n",
      "1            893         1       3   \n",
      "2            894         0       2   \n",
      "3            895         0       3   \n",
      "4            896         1       3   \n",
      "..           ...       ...     ...   \n",
      "413         1305         0       3   \n",
      "414         1306         1       1   \n",
      "415         1307         0       3   \n",
      "416         1308         0       3   \n",
      "417         1309         0       3   \n",
      "\n",
      "                                             Name     Sex   Age  SibSp  Parch  \\\n",
      "0                                Kelly, Mr. James    male  34.5      0      0   \n",
      "1                Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
      "2                       Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
      "3                                Wirz, Mr. Albert    male  27.0      0      0   \n",
      "4    Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
      "..                                            ...     ...   ...    ...    ...   \n",
      "413                            Spector, Mr. Woolf    male   NaN      0      0   \n",
      "414                  Oliva y Ocana, Dona. Fermina  female  39.0      0      0   \n",
      "415                  Saether, Mr. Simon Sivertsen    male  38.5      0      0   \n",
      "416                           Ware, Mr. Frederick    male   NaN      0      0   \n",
      "417                      Peter, Master. Michael J    male   NaN      1      1   \n",
      "\n",
      "                 Ticket      Fare Cabin Embarked  \n",
      "0                330911    7.8292   NaN        Q  \n",
      "1                363272    7.0000   NaN        S  \n",
      "2                240276    9.6875   NaN        Q  \n",
      "3                315154    8.6625   NaN        S  \n",
      "4               3101298   12.2875   NaN        S  \n",
      "..                  ...       ...   ...      ...  \n",
      "413           A.5. 3236    8.0500   NaN        S  \n",
      "414            PC 17758  108.9000  C105        C  \n",
      "415  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
      "416              359309    8.0500   NaN        S  \n",
      "417                2668   22.3583   NaN        C  \n",
      "\n",
      "[418 rows x 12 columns]\n",
      "\n",
      "\n",
      "DataFrame after filling missing values with 0:\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "0            892         0       3   \n",
      "1            893         1       3   \n",
      "2            894         0       2   \n",
      "3            895         0       3   \n",
      "4            896         1       3   \n",
      "..           ...       ...     ...   \n",
      "413         1305         0       3   \n",
      "414         1306         1       1   \n",
      "415         1307         0       3   \n",
      "416         1308         0       3   \n",
      "417         1309         0       3   \n",
      "\n",
      "                                             Name     Sex   Age  SibSp  Parch  \\\n",
      "0                                Kelly, Mr. James    male  34.5      0      0   \n",
      "1                Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
      "2                       Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
      "3                                Wirz, Mr. Albert    male  27.0      0      0   \n",
      "4    Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
      "..                                            ...     ...   ...    ...    ...   \n",
      "413                            Spector, Mr. Woolf    male   0.0      0      0   \n",
      "414                  Oliva y Ocana, Dona. Fermina  female  39.0      0      0   \n",
      "415                  Saether, Mr. Simon Sivertsen    male  38.5      0      0   \n",
      "416                           Ware, Mr. Frederick    male   0.0      0      0   \n",
      "417                      Peter, Master. Michael J    male   0.0      1      1   \n",
      "\n",
      "                 Ticket      Fare Cabin Embarked  \n",
      "0                330911    7.8292     0        Q  \n",
      "1                363272    7.0000     0        S  \n",
      "2                240276    9.6875     0        Q  \n",
      "3                315154    8.6625     0        S  \n",
      "4               3101298   12.2875     0        S  \n",
      "..                  ...       ...   ...      ...  \n",
      "413           A.5. 3236    8.0500     0        S  \n",
      "414            PC 17758  108.9000  C105        C  \n",
      "415  SOTON/O.Q. 3101262    7.2500     0        S  \n",
      "416              359309    8.0500     0        S  \n",
      "417                2668   22.3583     0        C  \n",
      "\n",
      "[418 rows x 12 columns]\n",
      "\n",
      "\n",
      "DataFrame after removing duplicates:\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "0            892         0       3   \n",
      "1            893         1       3   \n",
      "2            894         0       2   \n",
      "3            895         0       3   \n",
      "4            896         1       3   \n",
      "..           ...       ...     ...   \n",
      "413         1305         0       3   \n",
      "414         1306         1       1   \n",
      "415         1307         0       3   \n",
      "416         1308         0       3   \n",
      "417         1309         0       3   \n",
      "\n",
      "                                             Name     Sex   Age  SibSp  Parch  \\\n",
      "0                                Kelly, Mr. James    male  34.5      0      0   \n",
      "1                Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
      "2                       Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
      "3                                Wirz, Mr. Albert    male  27.0      0      0   \n",
      "4    Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
      "..                                            ...     ...   ...    ...    ...   \n",
      "413                            Spector, Mr. Woolf    male   0.0      0      0   \n",
      "414                  Oliva y Ocana, Dona. Fermina  female  39.0      0      0   \n",
      "415                  Saether, Mr. Simon Sivertsen    male  38.5      0      0   \n",
      "416                           Ware, Mr. Frederick    male   0.0      0      0   \n",
      "417                      Peter, Master. Michael J    male   0.0      1      1   \n",
      "\n",
      "                 Ticket      Fare Cabin Embarked  \n",
      "0                330911    7.8292     0        Q  \n",
      "1                363272    7.0000     0        S  \n",
      "2                240276    9.6875     0        Q  \n",
      "3                315154    8.6625     0        S  \n",
      "4               3101298   12.2875     0        S  \n",
      "..                  ...       ...   ...      ...  \n",
      "413           A.5. 3236    8.0500     0        S  \n",
      "414            PC 17758  108.9000  C105        C  \n",
      "415  SOTON/O.Q. 3101262    7.2500     0        S  \n",
      "416              359309    8.0500     0        S  \n",
      "417                2668   22.3583     0        C  \n",
      "\n",
      "[418 rows x 12 columns]\n",
      "\n",
      "\n",
      "DataFrame before data type conversion:\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('tested.csv')\n",
    "\n",
    "# Handling missing data\n",
    "print(\"DataFrame before handling missing values:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "print(\"DataFrame after filling missing values with 0:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"DataFrame after removing duplicates:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Data type conversions\n",
    "print(\"DataFrame before data type conversion:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf18e0e-440b-4536-85e8-9587a314be10",
   "metadata": {},
   "source": [
    "# Data analysis with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bd07f0d-fecd-4226-94c2-164f773b2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of the employee dataset:\n",
      "        Education  JoiningYear       City  PaymentTier          Age Gender  \\\n",
      "count        2764  2764.000000       2764  2764.000000  2764.000000   2764   \n",
      "unique          3          NaN          3          NaN          NaN      2   \n",
      "top     Bachelors          NaN  Bangalore          NaN          NaN   Male   \n",
      "freq         1971          NaN       1171          NaN          NaN   1529   \n",
      "mean          NaN  2015.090449        NaN     2.636035    30.952967    NaN   \n",
      "std           NaN     1.885943        NaN     0.624001     5.108872    NaN   \n",
      "min           NaN  2012.000000        NaN     1.000000    22.000000    NaN   \n",
      "25%           NaN  2013.000000        NaN     2.000000    27.000000    NaN   \n",
      "50%           NaN  2015.000000        NaN     3.000000    30.000000    NaN   \n",
      "75%           NaN  2017.000000        NaN     3.000000    35.000000    NaN   \n",
      "max           NaN  2018.000000        NaN     3.000000    41.000000    NaN   \n",
      "\n",
      "       EverBenched  ExperienceInCurrentDomain   LeaveOrNot  \n",
      "count         2764                2764.000000  2764.000000  \n",
      "unique           2                        NaN          NaN  \n",
      "top             No                        NaN          NaN  \n",
      "freq          2403                        NaN          NaN  \n",
      "mean           NaN                   2.644356     0.393632  \n",
      "std            NaN                   1.610610     0.488643  \n",
      "min            NaN                   0.000000     0.000000  \n",
      "25%            NaN                   1.000000     0.000000  \n",
      "50%            NaN                   2.000000     0.000000  \n",
      "75%            NaN                   4.000000     1.000000  \n",
      "max            NaN                   7.000000     1.000000  \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[1;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[1;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[0;32m   2456\u001b[0m     )\n\u001b[0;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   6548\u001b[0m ):\n\u001b[1;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m  12422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6456\u001b[0m     )\n\u001b[1;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[1;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py:1701\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[1;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert string 'BachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsMastersMastersPHDBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDMastersMastersMastersBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsPHDMastersMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsPHDMastersMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersMastersMastersMastersPHDBachelorsBachelorsBachelorsMastersMastersPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersMastersBachelorsMastersMastersMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsPHDMastersMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersPHDPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersMastersBachelorsMastersBachelorsPHDBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDPHDBachelorsPHDBachelorsMastersMastersMastersMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsPHDMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsMastersBachelorsPHDBachelorsBachelorsMastersBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsPHDMastersBachelorsPHDMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsMastersMastersBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsPHDBachelorsBachelorsBachelorsMastersMastersMastersBachelorsMastersMastersMastersMastersMastersMastersBachelorsBachelorsMastersMastersBachelorsPHDMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsMastersMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsPHDMastersMastersMastersBachelorsMastersBachelorsMastersMastersBachelorsPHDMastersMastersBachelorsPHDMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersMastersMastersBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsPHDPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersMastersBachelorsMastersBachelorsBachelorsMastersPHDBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsMastersMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsMastersPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsMastersPHDBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsPHDMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsPHDBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersPHDBachelorsBachelorsBachelorsBachelorsPHDMastersBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsPHDBachelorsMastersBachelorsBachelorsMastersBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsPHDBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsPHDBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDMastersBachelorsBachelorsBachelorsMastersPHDBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersPHDBachelorsBachelorsMastersBachelorsBachelorsBachelorsPHDMastersBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersPHDBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsMastersMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersMastersBachelorsBachelorsMastersBachelorsBachelorsMastersPHDMastersBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersPHDBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsMastersMastersMastersPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsMastersMastersBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsMastersBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsBachelorsBachelorsMastersBachelorsMastersBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsMastersBachelorsBachelorsMastersMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsBachelorsPHDBachelorsBachelorsPHDBachelorsMastersPHDBachelorsBachelorsBachelorsBachelorsBachelorsMastersMastersBachelorsMastersBachelorsMastersBachelorsPHDBachelorsBachelorsBachelorsMastersBachelorsBachelorsBachelorsMasters' to numeric",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Grouping by\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m grouped_by_sex\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEducation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Age and Education by Gender:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_by_Gender)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[1;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[0;32m   2446\u001b[0m         grouped_mean,\n\u001b[0;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[0;32m   2448\u001b[0m         engine_kwargs,\n\u001b[0;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2450\u001b[0m     )\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[0;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[1;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[43msb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_py_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[1;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[0;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[1;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "567543bd-b6d6-4fe9-abbf-b3a710c74617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of the Titanic dataset:\n",
      "        PassengerId    Survived      Pclass              Name   Sex  \\\n",
      "count    418.000000  418.000000  418.000000               418   418   \n",
      "unique          NaN         NaN         NaN               418     2   \n",
      "top             NaN         NaN         NaN  Kelly, Mr. James  male   \n",
      "freq            NaN         NaN         NaN                 1   266   \n",
      "mean    1100.500000    0.363636    2.265550               NaN   NaN   \n",
      "std      120.810458    0.481622    0.841838               NaN   NaN   \n",
      "min      892.000000    0.000000    1.000000               NaN   NaN   \n",
      "25%      996.250000    0.000000    1.000000               NaN   NaN   \n",
      "50%     1100.500000    0.000000    3.000000               NaN   NaN   \n",
      "75%     1204.750000    1.000000    3.000000               NaN   NaN   \n",
      "max     1309.000000    1.000000    3.000000               NaN   NaN   \n",
      "\n",
      "               Age       SibSp       Parch    Ticket        Fare  Cabin  \\\n",
      "count   418.000000  418.000000  418.000000       418  418.000000  418.0   \n",
      "unique         NaN         NaN         NaN       363         NaN   77.0   \n",
      "top            NaN         NaN         NaN  PC 17608         NaN    0.0   \n",
      "freq           NaN         NaN         NaN         5         NaN  327.0   \n",
      "mean     24.044258    0.447368    0.392344       NaN   35.541956    NaN   \n",
      "std      17.599608    0.896760    0.981429       NaN   55.867684    NaN   \n",
      "min       0.000000    0.000000    0.000000       NaN    0.000000    NaN   \n",
      "25%       9.000000    0.000000    0.000000       NaN    7.895800    NaN   \n",
      "50%      24.000000    0.000000    0.000000       NaN   14.454200    NaN   \n",
      "75%      35.750000    1.000000    0.000000       NaN   31.471875    NaN   \n",
      "max      76.000000    8.000000    9.000000       NaN  512.329200    NaN   \n",
      "\n",
      "       Embarked  \n",
      "count       418  \n",
      "unique        3  \n",
      "top           S  \n",
      "freq        270  \n",
      "mean        NaN  \n",
      "std         NaN  \n",
      "min         NaN  \n",
      "25%         NaN  \n",
      "50%         NaN  \n",
      "75%         NaN  \n",
      "max         NaN  \n",
      "\n",
      "\n",
      "Mean Age and Fare by Sex:\n",
      "              Age       Fare\n",
      "Sex                         \n",
      "female  25.293355  49.747699\n",
      "male    23.330489  27.424389\n",
      "\n",
      "\n",
      "Survival Rate by Passenger Class and Sex:\n",
      "Pclass  Sex   \n",
      "1       female    1.0\n",
      "        male      0.0\n",
      "2       female    1.0\n",
      "        male      0.0\n",
      "3       female    1.0\n",
      "        male      0.0\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "\n",
      "Merged DataFrame (with additional data):\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0          892         0       3   \n",
      "1          893         1       3   \n",
      "2          894         0       2   \n",
      "3          895         0       3   \n",
      "4          896         1       3   \n",
      "\n",
      "                                           Name     Sex   Age  SibSp  Parch  \\\n",
      "0                              Kelly, Mr. James    male  34.5      0      0   \n",
      "1              Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
      "2                     Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
      "3                              Wirz, Mr. Albert    male  27.0      0      0   \n",
      "4  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
      "\n",
      "    Ticket     Fare Cabin Embarked CabinClass  \n",
      "0   330911   7.8292     0        Q        NaN  \n",
      "1   363272   7.0000     0        S        NaN  \n",
      "2   240276   9.6875     0        Q        NaN  \n",
      "3   315154   8.6625     0        S        NaN  \n",
      "4  3101298  12.2875     0        S        NaN  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary statistics of the Titanic dataset:\")\n",
    "print(df.describe(include='all'))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Grouping by\n",
    "grouped_by_sex=df.groupby('Sex')[['Age', 'Fare']].mean()\n",
    "\n",
    "print(\"Mean Age and Fare by Sex:\")\n",
    "print(grouped_by_sex)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Grouping by\n",
    "survival_rate=df.groupby(['Pclass', 'Sex'])['Survived'].mean()\n",
    "\n",
    "print(\"Survival Rate by Passenger Class and Sex:\")\n",
    "print(survival_rate)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Creating a new DataFrame with additional data\n",
    "additional_data = pd.DataFrame({\n",
    "    'PassengerId': [1, 2, 3, 4, 5],\n",
    "    'CabinClass': ['First', 'Second', 'Third', 'First', 'Third']\n",
    "})\n",
    "\n",
    "# Merging\n",
    "merged_df=pd.merge(df, additional_data, left_on='PassengerId', right_on='PassengerId', how='left')\n",
    "print(\"Merged DataFrame (with additional data):\")\n",
    "print(merged_df.head())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0aef8-6da6-4231-b6a0-822f0fdea807",
   "metadata": {},
   "source": [
    "# Application in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ebdf9-9dff-4874-bba1-6e902a047997",
   "metadata": {},
   "source": [
    "Pandas is one of the most widely used libraries in data science due to its powerful data manipulation and analysis capabilities. Here are some key applications of Pandas in data science:\n",
    "\n",
    "1. Data Cleaning and Preprocessing\n",
    "Handling Missing Data: Pandas provides functions like dropna(), fillna(), and interpolate() to handle missing data efficiently. This is crucial because real-world datasets often have missing or incomplete entries that need to be addressed before analysis.\n",
    "Data Transformation: Pandas allows for easy transformation of data, such as converting data types, normalizing or standardizing values, and creating new columns based on existing data. These transformations are vital for preparing data for machine learning models.\n",
    "Removing Duplicates: The drop_duplicates() function helps in removing duplicate records, which can skew analysis results if left unchecked.\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "Summary Statistics: Using functions like describe(), mean(), median(), and std(), Pandas helps in quickly generating descriptive statistics that give an overview of the datas distribution and central tendencies.\n",
    "Data Visualization: While Pandas is not a visualization library, it integrates well with libraries like Matplotlib and Seaborn, allowing for easy creation of plots like histograms, bar charts, and box plots directly from DataFrames. This aids in identifying patterns, trends, and outliers.\n",
    "Correlation Analysis: With the corr() function, Pandas allows you to compute pairwise correlation between features, which is essential for understanding relationships between variables in your dataset.\n",
    "3. Data Wrangling\n",
    "Merging and Joining Datasets: Pandas provides powerful functions like merge() and join() to combine datasets based on common keys or indexes, enabling complex data manipulations needed when working with relational data.\n",
    "Grouping and Aggregation: Functions like groupby() and agg() allow for grouping data based on specific criteria and applying aggregate functions like sum, mean, or count. This is especially useful in summarizing large datasets by categories, such as sales data by region or time period.\n",
    "Pivot Tables: Pandas enables the creation of pivot tables using the pivot_table() function, which is invaluable for restructuring and summarizing data, similar to Excel pivot tables but with more flexibility.\n",
    "4. Time Series Analysis\n",
    "Resampling: Pandas supports time series data through functions like resample(), which allows for changing the frequency of your time series data (e.g., from daily to monthly), and rolling(), which provides rolling window calculations for moving averages or other statistics.\n",
    "DateTime Handling: Pandas makes it easy to work with date and time data through its DateTimeIndex, parsing dates from strings, and performing time zone conversions. This is crucial in industries like finance, where time series data is prevalent.\n",
    "5. Data Input and Output\n",
    "Reading and Writing Files: Pandas supports a wide range of data formats, including CSV, Excel, SQL databases, JSON, and more. The read_* and to_* functions make it easy to load data into DataFrames from various sources and export the results after processing.\n",
    "Web Scraping: Pandas can be used in conjunction with libraries like BeautifulSoup or Scrapy to scrape data from websites and directly load it into DataFrames for analysis.\n",
    "6. Machine Learning Integration\n",
    "Feature Engineering: Pandas is frequently used for creating new features from existing data, such as combining columns, encoding categorical variables, or calculating statistical metrics. These features can then be fed into machine learning models.\n",
    "Data Preparation for Modeling: Before feeding data into machine learning algorithms, it needs to be cleaned, normalized, and split into training and testing sets. Pandas streamlines this process, ensuring that data is in the correct format and free of inconsistencies.\n",
    "7. Financial Analysis\n",
    "Portfolio Analysis: In finance, Pandas is used to calculate returns, risk metrics, and portfolio optimization. Its essential for handling time series data, calculating cumulative returns, and analyzing the performance of different assets.\n",
    "Risk Management: Pandas can be used to model various risk scenarios, calculate Value at Risk (VaR), and simulate financial events using historical data.\n",
    "8. Scientific Research\n",
    "Data Modeling and Simulation: In scientific research, Pandas is often used for data modeling, where researchers simulate experiments or model natural processes. It supports complex calculations and data transformations required in various scientific domains.\n",
    "Reproducible Analysis: With Pandas, researchers can easily document and reproduce their data analysis workflows, ensuring transparency and consistency in their work.\n",
    "9. Text Data Processing\n",
    "Tokenization and Cleaning: While Pandas itself is not a natural language processing (NLP) tool, it is often used in preprocessing text data. Tasks like tokenization, stop word removal, and word frequency analysis can be performed on text data within DataFrames.\n",
    "Sentiment Analysis: Pandas can be used to manage and analyze text data for sentiment analysis, where text is converted into numerical data that can be analyzed or fed into machine learning models.\n",
    "\n",
    "In summary, Pandas is an essential tool in data science for its robust data manipulation, cleaning, analysis, and preparation capabilities, making it indispensable for professionals working with data in various fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ae148-9ccd-4b65-98a7-1d963e531e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}